{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "In this notebook, you are going to use feature engineering techniques to prepare data in a number of different ways.  Depending on the method recommendation system that's built, you will have different data to prepare.\n",
    "\n",
    "* **Collaborative Filtering**\n",
    "    - Dense or Sparse user-item ratings matrix\n",
    "\n",
    "\n",
    "* **Demographic Recommenders**\n",
    "    - User information including numeric and categorical features\n",
    "    \n",
    "    \n",
    "* **Content Based Recommenders**\n",
    "    - Item information including numeric, categorical, and text related features\n",
    "    \n",
    "    \n",
    "* **Utility Based Recommenders**\n",
    "    - can use any of the features from collaborative filtering, demographic, or content data\n",
    "    \n",
    "    \n",
    "* **Knowledge Based Recommenders**\n",
    "    - uses any of the above, but includes additional user input to filter results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intro:** Read in and take a look at the data below. This data is available from [MovieTweetings](https://github.com/sidooms/MovieTweetings).  Following the link provides additional information about the data should you want to know more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "movies_dat = pd.read_csv('./data/movies.dat', sep='::', engine='python',\n",
    "                         header=None, names=['movie_id', 'movie_title', 'movie_genre'])\n",
    "users_dat = pd.read_csv('./data/users.dat', sep='::', engine='python', \\\n",
    "                        header=None, names=['user_id', 'twitter_id'])\n",
    "ratings_dat = pd.read_csv('./data/ratings.dat', sep='::', engine='python', \\\n",
    "                          header=None, names=['user_id', 'movie_id','rating','time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder of what ratings_dat looks like\n",
    "ratings_dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Of the below descriptions, which is the best description of the data structure associated with **ratings_dat**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_solution_part2 as sp\n",
    "\n",
    "a = \"ratings_dat is a sparse representation of user-item ratings\"\n",
    "b = \"ratings_dat is a dense representation of user-item ratings\"\n",
    "c = \"ratings_dat is neither nor dense representation of user-item ratings\"\n",
    "\n",
    "your_answer = #a\n",
    "\n",
    "sp.answer_one(your_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** If you created a dataframe with movie_ids as the columns, user_ids as the rows, and each value filled with the rating of that user-movie combination; this would be an example of what type of representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"ratings_dat is a sparse representation of user-item ratings\"\n",
    "b = \"ratings_dat is a dense representation of user-item ratings\"\n",
    "c = \"ratings_dat is neither nor dense representation of user-item ratings\"\n",
    "\n",
    "your_answer = #a\n",
    "\n",
    "sp.answer_two(your_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "Use [pandas pivot_table](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html) to create the matrix described in question 2.  If you want to see how the resulting matrix should look, the `df_ratings_pivot` dataframe in the following cell holds the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the ratings_dat to answer the next question\n",
    "\n",
    "ratings_pivot = #your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings_pivot = sp.create_pivot_df(ratings_dat)\n",
    "df_ratings_pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** In the previous section, you saw that the dense representation of `ratings_dat` didn't have any missing values.  However, now we have pivoted this table.  How many missing values are in this pivot of the data, and how should we deal with them in this matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (3286, 'fill missing with column average')\n",
    "b = (3286, 'fill missing with row average')\n",
    "c = (3286, \"don't fill missing values\")\n",
    "d = (6384294, 'fill missing with column average')\n",
    "e = (6384294, 'fill missing with row average')\n",
    "f = (6384294, \"don't fill missing values\")\n",
    "g = (\"none of the above\")\n",
    "\n",
    "your_answer = #a\n",
    "\n",
    "sp.answer_three(your_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Now that you have found the number of missing values in `df_ratings_pivot` (or your matching `ratings_pivot`), what percentage of the values aren't missing? (Round your answer to 4 digits following the decimal - Ex: 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this cell for your work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_answer = #0.0001\n",
    "\n",
    "sp.answer_four(your_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** The data for many recommendation systems might not have user-item ratings.  Instead, you would only know if a user interacted with an item.  In these cases, you would have a similar dense matrix (just the ratings would be removed).  \n",
    "\n",
    "Create a 0, 1 dataframe that would represent the sparse matrix for this situation.  Store the matrix in `binary_df` below.  You can check your result against `ans_binary_df`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your work here\n",
    "\n",
    "binary_df = # create your df here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chech your df against the solution - run this cell to check\n",
    "ans_binary_df = sp.create_binary_df(ratings_pivot)\n",
    "# check that both the solution and your created df have the same stats\n",
    "print(print(\"Your df has {} 1's and the solution has {} 1's\".format(binary_df.sum().sum(), ans_binary_df.sum().sum()))\n",
    "print(\"Your df has a shape of {} and the solution has shape: {}\".format(binary_df.shape, ans_binary_df.shape))\n",
    "\n",
    "print(\"Here is a header of what your df should look like:\")\n",
    "ans_binary_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** If your `user_id`s and `movie_id`s are both in order from smallest to largest, then you can run the cell below to see if your binary matrix matches the solution.  If `True` appears after running the below, they match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.check_binary_df(binary_df, ans_binary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra:** \n",
    "\n",
    "The above shows the different manipulations of the data you would have to perform in most cases of creating a recommendation system, as most systems use Collaborative Filtering.\n",
    "\n",
    "However, if you decide to create a recommendation system that incorporates specific user or item data, then it is important to know how to create features from categorical and text data.  \n",
    "\n",
    "In the remaining sections of this workbook, you will be given an opportunity to test out these skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Unfortunately, we aren't given any user information in this dataset.  However, we can create additional features associated with movie information.  Using `movies_dat`, create a dataframe like `new_movies_dat` below, where :\n",
    "- the year of the movie is in its own column\n",
    "- a separate column with either a 1 or 0 is provided for each genre-movie combo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at your dataframe here\n",
    "movies_dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_movies_dat = sp.clean_movies_dat(movies_dat)\n",
    "new_movies_dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your work here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task** In order to get some practice with text cleaning, we will need text related to movies.  One of the major pieces of text we could collect is the script for each movie.\n",
    "\n",
    "One of the most popular websites for holding the scripts of movies is https://www.imsdb.com/.  I modified a codebase from here https://github.com/AnnaVM/Project_Plotline/ in order to scrape ~1200 movie scripts.  \n",
    "\n",
    "By `pip` installing `selenium`, you can run the `scraping_script.py` to do the same.  You may also need to pip install `BeautifulSoup`.\n",
    "\n",
    "If you scrape the articles, you will see the chrome browser move to each page as it is scraped, and at the end should have a path that looks like:\n",
    "\n",
    "<img src=\"../../images/script_paths.png\" width=\"200\" height=\"400\">\n",
    "\n",
    "**Task:** Once all the articles are scraped, create `script_dict` using the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running this cell will create give you script_dict, which\n",
    "# has a key associated with each file name, and value of the movie\n",
    "# script in text\n",
    "import os\n",
    "\n",
    "def create_script_dict(script_path='./data/scraping/texts/'):\n",
    "    '''\n",
    "    script_path is the path to the .txt files\n",
    "    \n",
    "    script_dict is a dictionary that holds the .txt file name as the key\n",
    "    and the contents of the file as the value\n",
    "    '''\n",
    "    \n",
    "    return script_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Perform common tasks in feature engineering text data:\n",
    "\n",
    "* remove punctuation, parantheses, brackets, etc.\n",
    "* make all words lower case\n",
    "* remove stop words\n",
    "* lemmatization of words\n",
    "* create a tf-idf matrix\n",
    "\n",
    "This process is well outlined in the article here: https://machinelearningmastery.com/clean-text-machine-learning-python/.\n",
    "\n",
    "You will want to `pip` install `nltk` to complete this task, which may take some time to complete.\n",
    "\n",
    "```\n",
    "sudo pip install -U nltk\n",
    "```\n",
    "\n",
    "If you aren't able to perform some tasks, you may need to download the data associated with them using `.download()`.\n",
    "\n",
    "```\n",
    "import nltk\n",
    "nltk.download()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "def create_clean_script_results(script_dict):\n",
    "    '''\n",
    "    script_dict has keys as the the .txt file name\n",
    "    and values as the text associated with the document\n",
    "    '''\n",
    "    # create empty dict to store results\n",
    "    clean_scripts = dict()\n",
    "    \n",
    "    for k, v in script_dict.items():    \n",
    "        # get tokens\n",
    "        \n",
    "\n",
    "        # create lower case words - remove punctuation\n",
    "        \n",
    "\n",
    "        # remove stop words and lemmatize\n",
    "        \n",
    "        \n",
    "        # store the key and string with spaces separating \n",
    "        # each of the tokens in clean_scripts\n",
    "        \n",
    "        \n",
    "    return clean_scripts\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** The final part would then be to create a tf-idf representation of the values.  This part continued to crash when I used sklearn, so I suggest using a Spark implementation.  An example of this implementation is available here: \n",
    "```\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.feature import HashingTF\n",
    "from pyspark.mllib.feature import IDF\n",
    "\n",
    "sc = SparkContext(...)\n",
    "\n",
    "# Load documents (one per line).\n",
    "documents = sc.textFile(\"...\").map(lambda line: line.split(\" \"))\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(documents)\n",
    "\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "```\n",
    "\n",
    "The resource for this was found here: https://stackoverflow.com/questions/35857837/tfidf-memoryerror-how-to-avoid-this-issue.\n",
    "\n",
    "Here was the sklearn implementation that continued to die out:\n",
    "\n",
    "```\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "X = tfidf_transformer.fit_transform(clean_scripts.values())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
