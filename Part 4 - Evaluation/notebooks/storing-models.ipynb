{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing Your Model\n",
    "\n",
    "Now you have all the knowledge needed to go full circle on creating and putting your model in production. Let's put that knowledge to practice with some code!\n",
    "\n",
    "In the cell below is a replication of the code you put together in the previous notebook - this code:\n",
    "\n",
    "1. Creates **train** and **test** datasets\n",
    "2. Creates three models: \n",
    " * **model_factorization**\n",
    " * **model_popular**\n",
    " * **model_itemsim**\n",
    "\n",
    "Run the cell below to get started with these three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to read in the libraries and data needed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import turicreate as tc\n",
    "import solution_part3 as sp\n",
    "\n",
    "ratings_dat = pd.read_csv('../../data/ratings.dat', sep='::', engine='python', \\\n",
    "                          header=None, names=['user_id', 'movie_id','rating','time'])\n",
    "\n",
    "ratings_dat2 = ratings_dat.copy(deep=True)\n",
    "ratings_dat2.columns = ['user_id', 'item_id', 'rating', 'time']\n",
    "ratings_sframe = tc.SFrame(ratings_dat2[['user_id', 'item_id', 'rating']])\n",
    "\n",
    "train, test = tc.recommender.util.random_split_by_user(ratings_sframe, \n",
    "                                                       user_id = 'user_id',\n",
    "                                                       item_id = 'item_id',\n",
    "                                                       max_num_users=None)\n",
    "\n",
    "# creating your three models of interest\n",
    "model_factorization = tc.factorization_recommender.create(train, target='rating')\n",
    "model_popular = tc.popularity_recommender.create(train, target='rating')\n",
    "model_itemsim = tc.item_similarity_recommender.create(train, target='rating',  similarity_type='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `rating` is being used, you will notice the metric being used is `RMSE`.  Use the [`evaluate_rmse`](https://apple.github.io/turicreate/docs/api/generated/turicreate.recommender.factorization_recommender.FactorizationRecommender.evaluate_rmse.html?highlight=evaluate_rmse#turicreate.recommender.factorization_recommender.FactorizationRecommender.evaluate_rmse) method of each of the 3 above models to compare how well each model performs on the `train` data.  \n",
    "\n",
    "Then answer the following question regarding your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Based on the results, which of the following are True?  \n",
    "\n",
    "**Add all of the True items statements to the `your_answer` list.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"the HIGHER the rmse, the BETTER the recommender\"\n",
    "b = \"using the train results, the best model is the popularity model\"\n",
    "c = \"using the train results, the best model is the item similarity model\"\n",
    "d = \"using the train results, the best model is the matrix factorization model\"\n",
    "e = \"the recommender that works best for the training data is the one we should use in the real world\"\n",
    "\n",
    "\n",
    "your_answer = #[a, b, c, d, e]\n",
    "\n",
    "sp.answer_one(your_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have looked at how well each model fits the `train`ing data, `evaluate` how well each model works on the `test` data.  Use your results to answer the following question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is one example, look at the others\n",
    "model_factorization.evaluate_rmse(test, target='rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Based on the results, which of the following are True?  \n",
    "\n",
    "**Add all of the True items statements to the `your_answer` list.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"using the test results, the best model is the popularity model\"\n",
    "b = \"using the test results, the best model is the item similarity model\"\n",
    "c = \"using the test results, the best model is the matrix factorization model\"\n",
    "d = \"the recommender that works best for the test data is the one we should use in the real world\"\n",
    "\n",
    "your_answer = #[a, b, c, d]\n",
    "\n",
    "sp.answer_two(your_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a situation in which you only know if an individual watched a movie or not, but you don't know the rating.  Below a new `ratings_dat` is created with a removed rating.  The training and testing data is again created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_sframe = tc.SFrame(ratings_dat2[['user_id', 'item_id']])\n",
    "\n",
    "train, test = tc.recommender.util.random_split_by_user(ratings_sframe, \n",
    "                                                       user_id = 'user_id',\n",
    "                                                       item_id = 'item_id',\n",
    "                                                       max_num_users=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the space below to **create** each of the same models as was done above using the **train** data, but instead of using the ratings, you will only use the user-item interactions.  The three types of models you should create include:\n",
    "\n",
    "1. `ranking_factorization_recommender` \n",
    "2. `popularity_recommender`\n",
    "3. `item_similarity_recommender`\n",
    "\n",
    "**Notice:** the `ranking_factorization_recommender` is needed in the cases of having classification data, rather than `factorization_recommender` which is used with ratings (regression) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating your three models of interest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since only the user-item relationships are being used, not ratings, you will notice `RMSE` is not used.  Instead, you will want to look at metrics associated with classification problems.\n",
    "\n",
    "You may remember from earlier sections some of these metrics include **precision**, **recall**, and **f1-scores**.  You will then use the [`evaluate`](https://apple.github.io/turicreate/docs/api/generated/turicreate.recommender.factorization_recommender.FactorizationRecommender.evaluate.html) method of each of the 3 above models to compare how well each model performs on the `test` data.  \n",
    "\n",
    "The results for each model are based on a `cutoff` value. Depending on which metric you would like to optimize on, you can choose a different cutoff.  Notice that by increasing the **precision**, you decrease the **recall** (and vice-versa).\n",
    "\n",
    "Use the below slots to take a look at the precision-recall values for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 1\n",
    "results_popular = model_popular.evaluate(test)\n",
    "results_popular['precision_recall_overall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Write a function that takes in the dataframe from `results['precision_recall_overall']` and adds a column for `f1_score` for each `cutoff`. You may find the [wiki page](https://en.wikipedia.org/wiki/F1_score) helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_f1score(df):\n",
    "    '''\n",
    "    input:\n",
    "        df: dataframe with cutoff, precision, and recall\n",
    "    \n",
    "    return:\n",
    "        df: datafra,e with cutoff, precision, recall, and f1_score\n",
    "    '''\n",
    "    # your code here\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try your function out\n",
    "create_f1score(results_popular['precision_recall_overall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Using each of your models and looking at the **f1-score** for each of the test sets, is the precision-recall consistent with your findings using rmse in terms of which modeling technique you should use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"the most popular recommender is the best, which matches what we got from rmse test\"\n",
    "b = \"the item similarity recommender is best, which does not match what we got from rmse test\"\n",
    "c = \"the factorization recommender is best, which does not match we got from rmse test\"\n",
    "d = \"we can't be sure based on the results which is best\"\n",
    "\n",
    "your_answer = #a\n",
    "\n",
    "sp.answer_four(your_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** Precision in this case means ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"of all the movies, the propotion you got right as watching or right as not watching\"\n",
    "b = \"of the movies we recommended, the proportion they actually watched\"\n",
    "c = \"of the movies that they actually watched, the proportion you recommended\"\n",
    "d = \"none of the above\"\n",
    "\n",
    "your_answer = #a\n",
    "\n",
    "sp.answer_five(your_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:** Recall in this case means..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"of all the movies, the propotion you got right as watching or right as not watching\"\n",
    "b = \"of the movies we recommended, the proportion they actually watched\"\n",
    "c = \"of the movies that they actually watched, the proportion you recommended\"\n",
    "d = \"none of the above\"\n",
    "\n",
    "your_answer = #a\n",
    "\n",
    "sp.answer_six(your_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have found the best model based on the test set, it is important that you make sure your model performs well in the real world.  In order to re-use models for new situations, you will want to save them.  Look at the [`save`](https://apple.github.io/turicreate/docs/userguide/recommender/using-trained-models.html) method at the bottom of the page here, and use it to save one of your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put some recommendations here, so you can compare with the loaded model\n",
    "new_user = tc.SFrame({'user_id': [0]})\n",
    "model_popular.recommend(new_user, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = './factor_model.model' # save your model here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the load method **link** and use it to load your existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model here and make sure the recommendations match from before\n",
    "loaded_model = # load your model here\n",
    "loaded_model.recommend(new_user, k=3) # test it has the same predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also want to do as you did earlier and store the results in a `json` format to be used by other engineering groups.  You could then imagine updating data files, re-creating your models, and then final creating new predictions.  This is a process you can find in the extra section to simulate how this might work in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this for good measure\n",
    "sp.end_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
