# Part 4 - Evaluation Methods

In Part 3, you had an opportunity to try a number of popular techniques for making recommendations.  You learned how to `create` a recommender, as well `recommend` new items to a user.

At this point, you have a number of skills and useful techniques available to make recommendations.  However, there are still a number of outstanding questions to answer in order to operationalize your model.  These questions include:
* How do we know if your recommender is "good"?
* How do we decide which recommender is "best"?
* How do we store the "best" recommender to be used in the future?

* How do we make recommendations when we have a new user? (the cold start problem)

Following this section, you will be able to answer each of these questions.


#### Terms & Concepts
- Evaluating your recommendations
    - train-test split
    - classification metrics
    - regression metrics
    - cross-validation
    - offline vs. online evaluation methods
- The cold start problem
- Storing your model(s) for future use

#### Navigating this Section

- [Slide Set 1: Model Evaluation](https://github.com/jbernhard-nw/rec-workshop/blob/master/Part%204%20-%20Evaluation/slides/PartV_Evaluation_Methods.pdf)
    - [Notebook 1: Model Evaluation](./notebooks/evaluation-methods.ipynb)
    - [Notebook 1: Model Evaluation - Solutions](./notebooks/solutions/evaluation-methods-Solution.ipynb)
- [Slide Set 2: The Cold Start Problem](./slides/PartV_ColdStart.pdf)
    - [Notebook 2: The Cold Start Problem](./notebooks/cold-start.ipynb)
    - [Notebook 2: The Cold Start Problem - Solutions](./notebooks/solutions/cold-start-Solution.ipynb)
    - [Notebook 3: Storing Your Model(s) For Future Use](./notebooks/storing-models.ipynb)
    - [Notebook 3: Storing Your Model(s) For Future Use - Solutions](./notebooks/solution/storing-models-Solution.ipynb)
