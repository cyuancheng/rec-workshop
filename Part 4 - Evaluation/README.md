# Part 4 - Evaluation Methods

In Part 3, you had an opportunity to try a number of popular techniques for making recommendations.  You learned how to `create` a recommender, as well `recommend` new items to a user.

At this point, you have a number of skills and useful techniques available to make recommendations.  However, there are still a number of outstanding questions to answer in order to operationalize your model.  These questions include:
* How do we know if your recommender is "good"?
* How do we decide which recommender is "best"?
* How do we store the "best" recommender to be used in the future?

* How do we make recommendations when we have a new user? (the cold start problem)

Following this section, you will be able to answer each of these questions.


#### Terms & Concepts
- Evaluating your recommendations
    - Train-test split
    - evaluation metrics
    - offline vs. online evaluation methods
- The cold start problem
- Storing your model(s) for future use

#### Navigating this Section

- [Slide Set 1: Model Evaluation]()
    - [Notebook 1: Model Evaluation]()
    - [Notebook 1: Model Evaluation - Solutions]()
- [Slide Set 2: The Cold Start Problem]()
    - [Notebook 2: The Cold Start Problem]()
    - [Notebook 2: The Cold Start Problem - Solutions]()
- [Slide Set 3: Storing Your Model(s) For Future Use]()
    - [Notebook 3: Storing Your Model(s) For Future Use]()
    - [Notebook 3: Storing Your Model(s) For Future Use - Solutions]()
